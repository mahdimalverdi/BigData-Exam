{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exam1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPveNTkKvipCyU46J6xKr0r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahdimalverdi/BigData-Exam/blob/main/Exam1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Bnt5cgG80N"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKhAhPqeHNHU"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT287R0QHQDY"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU8A9-4wH7NL"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType\n",
        "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
        "from pyspark.sql.functions import col,array_contains\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName(\"hw6\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtaKVHopS4Jw",
        "outputId": "d328e666-99b3-409a-8a48-9c5ca73dc1fd"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mahdimalverdi/BigData-Exam/main/select_20_features.csv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-22 12:48:06--  https://raw.githubusercontent.com/mahdimalverdi/BigData-Exam/main/select_20_features.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44041 (43K) [text/plain]\n",
            "Saving to: ‘select_20_features.csv’\n",
            "\n",
            "\rselect_20_features.   0%[                    ]       0  --.-KB/s               \rselect_20_features. 100%[===================>]  43.01K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2021-06-22 12:48:06 (7.31 MB/s) - ‘select_20_features.csv’ saved [44041/44041]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4dOFLq_w-Nw",
        "outputId": "e7ed2803-7840-4575-8a2e-7dcebc427878"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mahdimalverdi/BigData-Exam/main/labels.csv"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-22 12:48:50--  https://raw.githubusercontent.com/mahdimalverdi/BigData-Exam/main/labels.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1112 (1.1K) [text/plain]\n",
            "Saving to: ‘labels.csv’\n",
            "\n",
            "\rlabels.csv            0%[                    ]       0  --.-KB/s               \rlabels.csv          100%[===================>]   1.09K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-06-22 12:48:50 (27.1 MB/s) - ‘labels.csv’ saved [1112/1112]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCNPzToQJRRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d66b66-5cd9-48be-b0ec-b9a2f2fd75e4"
      },
      "source": [
        "train_dataset = spark.read.csv(\"select_20_features.csv\",inferSchema=True, header =False)\n",
        "train_dataset.printSchema()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _c0: double (nullable = true)\n",
            " |-- _c1: double (nullable = true)\n",
            " |-- _c2: double (nullable = true)\n",
            " |-- _c3: double (nullable = true)\n",
            " |-- _c4: double (nullable = true)\n",
            " |-- _c5: double (nullable = true)\n",
            " |-- _c6: double (nullable = true)\n",
            " |-- _c7: double (nullable = true)\n",
            " |-- _c8: double (nullable = true)\n",
            " |-- _c9: double (nullable = true)\n",
            " |-- _c10: double (nullable = true)\n",
            " |-- _c11: double (nullable = true)\n",
            " |-- _c12: double (nullable = true)\n",
            " |-- _c13: double (nullable = true)\n",
            " |-- _c14: double (nullable = true)\n",
            " |-- _c15: double (nullable = true)\n",
            " |-- _c16: double (nullable = true)\n",
            " |-- _c17: double (nullable = true)\n",
            " |-- _c18: double (nullable = true)\n",
            " |-- _c19: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dqJ_rwlXri6",
        "outputId": "a6e324e7-104c-442d-bbe1-d82b823c9d21"
      },
      "source": [
        "lable_dataset = spark.read.csv(\"labels.csv\",inferSchema=True, header =True)\n",
        "lable_dataset.printSchema()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- lable: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjOHkwHhZqjP"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, LongType\n",
        "\n",
        "def with_column_index(sdf): \n",
        "    new_schema = StructType(sdf.schema.fields + [StructField(\"ColumnIndex\", LongType(), False),])\n",
        "    return sdf.rdd.zipWithIndex().map(lambda row: row[0] + (row[1],)).toDF(schema=new_schema)\n",
        "\n",
        "df1_ci = with_column_index(lable_dataset)\n",
        "df2_ci = with_column_index(train_dataset)\n",
        "join_on_index = df1_ci.join(df2_ci, df1_ci.ColumnIndex == df2_ci.ColumnIndex, 'inner').drop(\"ColumnIndex\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B4HKse8K9tb"
      },
      "source": [
        "def getHeaders(n):\n",
        "  return n.name\n",
        "\n",
        "headers = list(map(getHeaders,train_dataset.schema.fields))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx52fgxMLRN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b10b30-03b7-472c-d2bd-f45723da6252"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "assembler = VectorAssembler(inputCols=headers,outputCol = 'Attributes')\n",
        "\n",
        "output = assembler.transform(join_on_index)\n",
        "finalized_data = output.select(\"Attributes\",\"lable\")\n",
        "finalized_data.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|          Attributes|lable|\n",
            "+--------------------+-----+\n",
            "|[0.75,1.0,0.5,0.5...|    1|\n",
            "|[0.75,1.0,0.5,0.5...|    1|\n",
            "|[1.0,1.0,0.25,0.5...|    3|\n",
            "|[0.75,1.0,0.5,0.5...|    1|\n",
            "|[0.75,1.0,0.5,0.5...|    1|\n",
            "|[1.0,0.5,0.5,0.75...|    2|\n",
            "|[0.75,0.5,0.5,0.2...|    0|\n",
            "|[1.0,0.25,0.25,1....|    3|\n",
            "|[0.75,1.0,0.5,0.5...|    1|\n",
            "|[0.25,0.5,0.5,0.5...|    1|\n",
            "|[0.75,1.0,0.5,0.5...|    1|\n",
            "|[0.75,1.0,0.5,0.5...|    1|\n",
            "|[1.0,0.75,0.5,0.5...|    3|\n",
            "|[0.25,0.75,0.5,0....|    2|\n",
            "|[0.75,1.0,0.5,0.5...|    1|\n",
            "|[1.0,0.5,0.25,0.5...|    1|\n",
            "|[0.75,1.0,0.5,0.5...|    1|\n",
            "|[1.0,0.5,0.5,1.0,...|    0|\n",
            "|[0.75,0.5,0.5,0.2...|    1|\n",
            "|[0.25,0.5,0.5,0.5...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv_qTHx1x2T2"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "def evaluate(result):\n",
        "  evaluator = MulticlassClassificationEvaluator(\n",
        "      labelCol=\"lable\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "  accuracy = evaluator.evaluate(result)\n",
        "  print(\"Test Error = %g \" % (1.0 - accuracy))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qbc9Jv2x5VU",
        "outputId": "e1c440ee-d2cb-4da4-d2b6-fec1b37b5361"
      },
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "regressor = DecisionTreeRegressor(featuresCol = 'Attributes', labelCol = 'lable')\n",
        "data  = finalized_data.randomSplit([10.0, 1.0], 24)\n",
        "regressor = regressor.fit(data[0])\n",
        "result = regressor.transform(data[1])\n",
        "result.show()\n",
        "evaluate(result)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+------------------+\n",
            "|          Attributes|lable|        prediction|\n",
            "+--------------------+-----+------------------+\n",
            "|[0.75,1.0,0.5,0.5...|    1|0.9775280898876404|\n",
            "|[1.0,0.5,0.25,0.5...|    1|1.0222222222222221|\n",
            "|[1.0,0.5,0.5,0.75...|    2|1.8846153846153846|\n",
            "|[1.0,0.5,0.25,0.5...|    1|1.0222222222222221|\n",
            "|[1.0,0.5,0.5,0.75...|    2|1.8846153846153846|\n",
            "|[0.25,0.75,0.5,0....|    2|1.8846153846153846|\n",
            "|[0.5,1.0,1.0,1.0,...|    2|               0.5|\n",
            "|[0.75,1.0,0.5,0.5...|    1|0.9775280898876404|\n",
            "|[1.0,1.0,0.75,1.0...|    3|0.6666666666666666|\n",
            "|[0.5,0.5,0.75,0.7...|    1|1.0222222222222221|\n",
            "|[1.0,0.5,0.5,0.75...|    2|1.8846153846153846|\n",
            "|[1.0,0.25,1.0,0.7...|    3|2.5714285714285716|\n",
            "|[0.75,0.5,0.75,1....|    2|               1.8|\n",
            "|[0.75,1.0,0.5,0.5...|    1|0.9775280898876404|\n",
            "|[1.0,0.5,0.5,0.75...|    2|1.8846153846153846|\n",
            "|[1.0,0.5,0.25,0.5...|    1|1.0222222222222221|\n",
            "|[0.5,0.25,0.5,0.7...|    1|1.8846153846153846|\n",
            "|[0.5,0.75,0.5,0.2...|    1|1.8846153846153846|\n",
            "|[0.5,0.25,0.5,0.5...|    2|2.4285714285714284|\n",
            "|[1.0,0.5,0.5,0.75...|    2|1.8846153846153846|\n",
            "+--------------------+-----+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Test Error = 0.978723 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}